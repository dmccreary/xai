{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Explainable AI Course","text":"<p>In this course, we will cover the key concepts around building analytical systems that focus on explainability.  This is an important goal for many modern AI systems today.  Studies show that people are reluctant to use AI systems that they don't trust, and explainability is a key to building that trust.</p> <p>Please let me know if you have any feedback on this course and our classroom materials.</p>"},{"location":"contact/","title":"Contact","text":"<p>Please contact me on LinkedIn</p> <p>Thanks! - Dan</p>"},{"location":"glossary/","title":"Glossary of Terms for Explainable AI","text":""},{"location":"glossary/#clever-hans","title":"Clever Hans","text":""},{"location":"glossary/#human-in-the-loop","title":"Human in the Loop","text":""},{"location":"glossary/#level-of-risk","title":"Level of Risk","text":"<p>In our course, we define five levels of risk related to making a prediction and needing a clear explanation of why a prediction was made.</p> <ol> <li>Passive Modification: AI agents perform a background process that modifies resources but does not involve high-stakes</li> <li>Assisted Decision-Making AI agents provide low-stakes recommendations where human confirmation is required before any action is taken.</li> <li>Opt-In High-Stakes: A high-stakes recommendation and humans must opt-in to accept a recommendation made by an AI agent</li> <li>Opt-Out High-Stakes: A high-stakes recommendation humans must opt-out to avoid a recommendation</li> <li>Autonomous High-Stakes: A high-stakes recommendation and there are no humans in the loop</li> </ol>"},{"location":"glossary/#lime","title":"LIME","text":""},{"location":"glossary/#locked-algorithm","title":"Locked Algorithm","text":"<p>An algorithm that provides the same result each time the same input is applied to it and does not change with use. Examples of locked algorithms are static look-up tables, decision trees, and complex classifiers.</p>"},{"location":"glossary/#shap","title":"SHAP","text":""},{"location":"how-i-built-this-site/","title":"How I Built This Site","text":""},{"location":"how-i-built-this-site/#cover-image-prompt","title":"Cover image prompt","text":"<p>Create an ultra-high-resolution detailed image of two cubes placed side-by-side on a dull gray surface. The cubes are exactly the same size and have identical dimensions for the viewer. The cube on the left is a transparent cube with a very colorful and complex network inside it. The cube on the right has an opaque glossy black surface that obscures anything within it and is completely devoid of any color.</p>"},{"location":"license/","title":"Creative Commons License","text":"<p>All content in this repository is governed by the following license agreement:</p>"},{"location":"license/#license-type","title":"License Type","text":"<p>Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED)</p>"},{"location":"license/#link-to-license-agreement","title":"Link to License Agreement","text":"<p>https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en</p>"},{"location":"license/#your-rights","title":"Your Rights","text":"<p>You are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format</li> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p>"},{"location":"license/#restrictions","title":"Restrictions","text":"<ul> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li> <li>NonCommercial \u2014 You may not use the material for commercial purposes.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> <li>No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.</li> </ul> <p>Notices</p> <p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.</p> <p>No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p> <p>This deed highlights only some of the key features and terms of the actual license. It is not a license and has no legal value. You should carefully review all of the terms and conditions of the actual license before using the licensed material.</p>"},{"location":"references/","title":"Explainable AI References","text":"<ol> <li> <p>Artificial Intelligence/Machine Learning (AI/ML)-Based.:Jf/&lt;X Software as a Medical Device (SaMD) Action Plan</p> </li> <li> <p>Proposed Regulatory Framework for Modifications to Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD) - Discussion Paper and Request for Feedback</p> </li> <li> <p>IMDRF SAMD Definitions</p> </li> <li> <p>Deciding When to Submit a 510(k) for a Software Change to an Existing Device</p> </li> </ol>"},{"location":"types-of-analysis/","title":"Types of Data Analysis Methods Ranked by Explainability","text":"<p>When evaluating the explainability of data analysis methods, it's important to consider how easily the results can be interpreted by humans. Here's an ordered list of the methods mentioned, along with a few additional common methods, ranked from most explainable to least explainable:</p> <ol> <li> <p>Decision Trees: Highly explainable due to their simple, rule-based structure that mimics human decision-making processes.</p> </li> <li> <p>Linear Regression: Highly interpretable because it models relationships linearly with clear coefficients.</p> </li> <li> <p>Logistic Regression: Similar to linear regression but used for classification problems. The coefficients provide direct insights.</p> </li> <li> <p>Knowledge Graph: Explainability depends on the complexity of the graph and the relationships modeled, but generally, they are quite interpretable due to their structured nature.</p> </li> <li> <p>Single Layer Neural Network: More complex than regression models but still relatively straightforward due to having only one layer of processing.</p> </li> <li> <p>Gradient Boosting with XGBoost: While feature importance can be extracted, the ensemble nature and complex interactions make it less interpretable than simpler models.</p> </li> <li> <p>Two Layer Deep Neural Network: Begins to enter the realm of deep learning, making it harder to interpret due to multiple layers of abstraction.</p> </li> <li> <p>Random Forest: An ensemble of decision trees, more accurate than a single tree but less interpretable due to averaging over many trees.</p> </li> <li> <p>Deep Neural Network (more than two layers): As the number of layers increases, so does the model's complexity, making it less interpretable.</p> </li> <li> <p>Convolutional Neural Networks (CNNs): Commonly used in image analysis. They are less interpretable due to their complex structure designed to pick up hierarchical patterns.</p> </li> <li> <p>Recurrent Neural Networks (RNNs): Especially ones with long short-term memory (LSTM) units. Used for sequential data like time series or text, they are quite complex and less interpretable.</p> </li> <li> <p>Transformer Models (like BERT, GPT): Very complex with millions of parameters, making them the least interpretable.</p> </li> </ol>"},{"location":"advanced/","title":"Advance Concepts","text":"<p>This section assumes you have a more technical background than our other chapters.  You should have a solid background in statistics and Python programming to use these sections.</p>"},{"location":"advanced/#shap","title":"SHAP","text":"<p>SHAP</p>"},{"location":"advanced/shap-conda-setup/","title":"SHAP Conda Setup","text":"<p>To set up a Conda environment specifically for using SHAP on a Mac, you can follow these UNIX shell commands. This assumes that you already have Conda installed on your system. If you don't have Conda installed, you can download it from Anaconda's official site.</p> <ol> <li> <p>Open Terminal: You can find the Terminal app in your Applications folder, under Utilities, or use Spotlight to search for it.</p> </li> <li> <p>Create a New Conda Environment:</p> </li> </ol> <pre><code>conda create --name shap_env python=3.8\n</code></pre> <p>Here, <code>shap_env</code> is the name of the new environment, and <code>python=3.8</code> specifies the Python version. You can choose the Python version as per your requirement.</p> <ol> <li>Activate the New Environment:</li> </ol> <p>Before you begin, if you are currently using conda we suggest you deactivate any conda environment you are using:</p> <pre><code>conda deactivate\n ```\n\n```sh\nconda activate shap_env\n ```\n\nAfter this command, you should see `(shap_env)` before the command prompt, indicating that the environment is active.\n\n4.  **Install SHAP**:\n\n```sh\nconda install -c conda-forge shap\n</code></pre> <p>This command installs SHAP from the Conda-Forge channel.</p> <ol> <li> <p>Install Additional Dependencies:</p> <ul> <li> <p>If you plan to use SHAP with machine learning models, you might also want to install relevant libraries. For example, to install <code>scikit-learn</code>:     <pre><code>bashCopy code\n`conda install -c conda-forge scikit-learn\n`\n</code></pre></p> </li> <li> <p>For data handling with <code>pandas</code>:     <pre><code>bashCopy code\n`conda install -c conda-forge pandas\n`\n</code></pre></p> </li> <li> <p>For array operations with <code>numpy</code>:     <pre><code>bashCopy code\n`conda install -c conda-forge numpy\n`\n</code></pre></p> </li> </ul> </li> <li> <p>Optional: Install Jupyter Notebook: If you want to use Jupyter notebooks:</p> <pre><code>bashCopy code\n`conda install -c conda-forge notebook\n`\n</code></pre> </li> <li> <p>Verify Installation:</p> <ul> <li> <p>You can verify the installation of SHAP and other packages by running Python in your new environment:     <pre><code>bashCopy code\n`python\n`\n</code></pre></p> </li> <li> <p>Then, in the Python interpreter, try importing SHAP:     <pre><code>pythonCopy code\n`import shap\n`\n</code></pre></p> </li> </ul> </li> </ol> <p>Remember, every time you start a new terminal session and want to work with SHAP, you should activate the Conda environment using <code>conda activate shap_env</code>. This ensures that you are using the correct Python version and dependencies isolated for your SHAP-related work.</p>"},{"location":"advanced/shap-python/","title":"SHAP for Python Developers","text":"<p>To set up a Python environment for using SHAP in a machine learning project, a programmer would typically follow these high-level steps:</p> <ol> <li> <p>Setting Up the Python Environment:</p> <ul> <li>Install Python: Ensure that Python is installed. Python 3.x is recommended as it's the latest version supported by most libraries.</li> <li>Create a Virtual Environment (optional but recommended): This helps in managing dependencies specific to the project without affecting the global Python setup. Tools like <code>venv</code> or <code>conda</code> can be used to create a virtual environment.</li> </ul> <p>In this course we will be using conda.</p> <p>Use the following steps 2.  Install Required Libraries:</p> <ul> <li>Install SHAP: Use pip, Python's package installer. The command is usually as simple as <code>pip install shap</code>.</li> <li>Install Machine Learning Libraries: Since SHAP is used to explain the output of machine learning models, you need to have machine learning libraries installed. Common ones include <code>scikit-learn</code> for general machine learning, <code>pandas</code> for data handling, and <code>numpy</code> for numerical operations. For deep learning, you might require <code>tensorflow</code> or <code>pytorch</code>.</li> <li>Install Visualization Libraries (optional): For visualizing SHAP values, libraries like <code>matplotlib</code> or <code>seaborn</code> might be needed.</li> <li> <p>Developing the Machine Learning Model:</p> </li> <li> <p>Preprocess the Data: Use libraries like <code>pandas</code> and <code>numpy</code> to load, clean, and prepare your data for modeling.</p> </li> <li>Build and Train the Model: Create a machine learning model using a library like <code>scikit-learn</code> or a deep learning framework.</li> <li> <p>Integrating SHAP:</p> </li> <li> <p>Import SHAP in Your Script: Include SHAP in your Python script by adding <code>import shap</code>.</p> </li> <li>Create a SHAP Explainer: This object is used to calculate SHAP values. The type of explainer depends on your model (e.g., <code>TreeExplainer</code> for tree-based models, <code>KernelExplainer</code> for more general models).</li> <li>Generate SHAP Values: Use the explainer to compute SHAP values for your model's predictions. This involves passing the feature data and sometimes the model itself to the explainer.</li> <li> <p>Analyzing SHAP Values:</p> </li> <li> <p>Interpret the Results: Use SHAP's visualization tools to interpret the SHAP values. Common plots include summary plots and dependence plots, which can be created using SHAP's built-in functions.</p> </li> <li>Incorporate Findings into Model Development: Use the insights gained from SHAP analysis to improve your model. This could involve feature selection, model tuning, or addressing data biases.</li> <li> <p>Maintaining the Project:</p> </li> <li> <p>Version Control: Use a version control system like Git to keep track of changes in your project.</p> </li> <li>Documentation: Document your setup and analysis process, which is crucial for reproducibility and collaboration.</li> <li> <p>Staying Updated:</p> </li> <li> <p>Regularly update your libraries to get the latest features and security updates. This can be done using <code>pip install --upgrade &lt;library&gt;</code>.</p> </li> </ul> </li> </ol> <p>By following these steps, a Python programmer can effectively set up and use SHAP in their machine learning projects to provide interpretability to their model's predictions.</p>"},{"location":"advanced/shap/","title":"SHAP (SHapley Additive exPlanations)","text":""},{"location":"advanced/shap/#shap-overview","title":"SHAP Overview**","text":"<p>SHAP is a powerful tool in the field of machine learning that helps in explaining the output of machine learning models. It is based on the concept of Shapley values, a method from cooperative game theory. SHAP provides insights into how each feature in a model contributes to the prediction for each individual data point, thereby offering a way to interpret complex machine learning models, especially those considered as \"black box\" models.</p>"},{"location":"advanced/shap/#how-shap-works","title":"How SHAP Works","text":"<ol> <li> <p>Shapley Values Concept: SHAP leverages Shapley values, originally developed for fair profit distribution in coalitions. In the context of machine learning, it considers each feature value of a data instance as a \"player\" in a game where the \"payout\" is the prediction made by the model.</p> </li> <li> <p>Feature Contribution Calculation: SHAP calculates the contribution of each feature to the difference between the actual prediction and the average prediction. The idea is to simulate all possible combinations of feature values and observe how the prediction changes when a feature is added to these combinations.</p> </li> <li> <p>Model Agnosticism: One of the strengths of SHAP is its model-agnostic nature. It can be applied to a wide range of machine learning models, from simple linear regression to complex deep learning models.</p> </li> <li> <p>Local Interpretation: SHAP focuses on local interpretability, providing explanations for individual predictions. This means it can explain why a model made a specific prediction for a specific instance, rather than offering a global view of the model's overall behavior.</p> </li> </ol>"},{"location":"advanced/shap/#using-shap-in-practice","title":"Using SHAP in Practice","text":"<ol> <li> <p>Model Debugging and Improvement: By understanding the impact of each feature on the model's predictions, practitioners can debug and improve the model's performance, particularly in identifying and fixing biases.</p> </li> <li> <p>Feature Importance Analysis: SHAP helps in understanding which features are most important for a model's decision-making process. This is crucial in many domains like finance and healthcare, where explaining the rationale behind decisions is as important as the decisions themselves.</p> </li> <li> <p>Compliance and Trust: In regulated industries, explaining model predictions is often necessary for compliance. SHAP aids in providing these explanations, thereby fostering trust among users and stakeholders.</p> </li> <li> <p>Data Science Insight: SHAP can reveal surprising patterns in the data that may not be immediately apparent, providing valuable insights to data scientists and domain experts.</p> </li> </ol>"},{"location":"advanced/shap/#conclusion","title":"Conclusion","text":"<p>SHAP stands out as a versatile and powerful tool for explaining machine learning models. Its ability to quantify the impact of each feature on a prediction makes it invaluable for improving model performance, ensuring fairness and bias mitigation, and complying with regulatory requirements. Its application is crucial for advancing the field of explainable AI, ensuring that machine learning models remain both powerful and interpretable.</p>"},{"location":"intro/","title":"What is Explainable AI?","text":"<p>Welcome to our course on Explainable AI (XAI), where we embark on a journey through the fascinating world of artificial intelligence (AI) that is not only powerful but also interpretable and transparent. This course is designed to unravel the complexities of AI, making it understandable and accountable to a wide range of stakeholders, from developers to end-users.</p>"},{"location":"intro/#defining-explainable-ai","title":"Defining Explainable AI","text":"<p>Explainable AI refers to methods and techniques in the field of artificial intelligence that make the outputs and operations of AI systems understandable to humans. Unlike traditional AI systems, where the decision-making process can be opaque, XAI aims to create a transparent relationship between the AI's functionality and its decision-making process.</p>"},{"location":"intro/#comparing-systems-simple-vs-deep-learning-systems","title":"Comparing Systems: Simple vs. Deep-Learning Systems","text":"<p>To grasp the essence of XAI, it's crucial to understand the contrast between simple, easy-to-explain systems and complex deep-learning systems. Simple AI systems, like decision trees, are inherently transparent with clear-cut rules and predictable outcomes. On the other hand, deep-learning systems, utilizing neural networks with multiple layers, are akin to a \"black box.\" They can process an immense amount of data and find patterns beyond human capabilities, but their internal workings are often not easily interpretable.</p>"},{"location":"intro/#white-box-vs-black-box-models","title":"White Box vs. Black Box Models","text":"<p>This brings us to the concepts of \"White Box\" and \"Black Box\" models in AI. White Box models are those where the internal logic is fully transparent and understandable to humans. In contrast, Black Box models are those where the decision-making process is not visible or understandable, making it challenging to decipher how the AI arrived at a specific decision.</p>"},{"location":"intro/#the-dangers-of-non-explainable-systems","title":"The Dangers of Non-Explainable Systems","text":"<p>The inability to understand or predict the behavior of AI systems poses significant risks. It can lead to a lack of trust, incorrect or unethical decision-making, and difficulties in identifying and correcting errors. In domains like healthcare, finance, and law, where decisions have profound impacts, the opaqueness of Black Box models can be particularly perilous.</p>"},{"location":"intro/#tools-to-classify-risk-in-ai-systems","title":"Tools to Classify Risk in AI Systems","text":"<p>To mitigate these risks, it's vital to have tools that can classify the risk associated with AI decisions. These tools assess factors like the impact of decisions, the complexity of models, and the possibility of bias, providing a framework to evaluate the potential harm or unintended consequences of AI decisions.</p>"},{"location":"intro/#making-deep-neural-networks-explainable","title":"Making Deep Neural Networks Explainable","text":"<p>Lastly, we will explore the tools and techniques to make deep neural networks more explainable. Techniques like Layer-wise Relevance Propagation (LRP), SHAP (SHapley Additive exPlanations), and attention mechanisms can help illuminate the inner workings of complex models. We will delve into how these tools break down the AI's decision-making process, making it more transparent and understandable.</p> <p>Throughout this course, we will explore these concepts in detail, equipping you with the knowledge to create, analyze, and advocate for AI systems that are not just powerful, but also responsible and explainable. Welcome aboard this exciting exploration into the world of Explainable AI.</p>"},{"location":"intro/02-clever-hans/","title":"Clever Hans","text":"<p>The Tale of Clever Hans, the \"Mathematical\" Horse</p> <p></p> <p>Once upon a time in Germany, around the early 1900s, there lived a horse named Hans. But he wasn't just any horse; he was \"Clever Hans,\" a horse with a knack for numbers---or so people thought. Hans was owned by Wilhelm von Osten, a school teacher and horse trainer who claimed that Hans could understand German and solve complex mathematical problems. Imagine a horse doing your math homework!</p> <p>Von Osten would ask Hans questions like \"What is two plus three?\" and Hans would tap his hoof five times. The crowd would go wild! \"Ein Genie!\" they exclaimed, which means \"A genius!\" in German. Hans became a celebrity, with people coming from far and wide to see the brainy horse who could count.</p> <p>But, as with all great mysteries, there was a twist. A psychologist named Oskar Pfungst became curious about Hans's abilities. Pfungst conducted a series of experiments and discovered something astonishing. It turned out Hans was not exactly a mathematician; instead, he was incredibly good at reading human body language!</p> <p>When von Osten or the audience knew the answer to a question, they would, without realizing it, give subtle, unintentional cues. A slight nod, a change in posture, or an eager look in their eyes. Hans was picking up on these tiny signals, and when the signals stopped---like when the audience held their breath in anticipation---he would stop tapping.</p> <p>Hans was not a math whiz, but he was a master of observation, a Sherlock Holmes of the equine world!</p> <p>Relating Clever Hans to Explainable AI</p> <p>The story of Clever Hans is a humorous yet enlightening prelude to the importance of explainable AI. Just like how people believed Hans understood math, we often assume AI systems make decisions based on logical processing of the given inputs. However, sometimes, these systems might be 'tapping their hoof' based on subtle, unintended patterns in the data, not unlike Hans responding to unconscious human cues.</p> <p>In AI, this is akin to a model picking up on noise, biases, or irrelevant patterns in the data, leading to correct outcomes for the wrong reasons. Just as Pfungst's investigation revealed the true nature of Hans's abilities, explainable AI aims to dissect and understand the 'how' and 'why' behind AI decisions. It's crucial to ensure AI systems are making decisions for the right reasons, especially in critical applications like healthcare, finance, and law.</p> <p>So, the next time you encounter a seemingly \"intelligent\" AI, remember Clever Hans and ask yourself, \"Is this AI genuinely smart, or is it just good at picking up on the digital equivalent of a nod and a wink?\" The quest for explainable AI is, in essence, the pursuit of ensuring our AI systems are more than just clever at tapping hooves, but truly understanding and solving the problems at hand.</p>"},{"location":"intro/02-clever-hans/#short-cut-learning","title":"Short-Cut Learning","text":"<p>\"Short-cut learning\" in the context of artificial intelligence and machine learning is a phenomenon where a model achieves the correct answer or output, but it does so for the wrong reasons. This concept is a bit like a student who correctly guesses the answers on a test without actually understanding the subject matter.</p> <p>In more technical terms, short-cut learning occurs when a machine learning model picks up on superficial, spurious, or irrelevant patterns in the training data that happen to correlate with the right answers. These patterns are not the underlying causal factors that a human expert would use to make a decision. As a result, while the model may perform well on the training data or even some testing scenarios, its understanding is shallow and may fail in real-world situations where the spurious patterns it learned do not apply.</p> <p>For example, imagine a medical AI trained to detect a certain disease from X-ray images. If most images of diseased lungs also coincidentally have a small marker or label on them that healthy lung images do not, the model might learn to identify the disease by looking for this marker instead of actual signs of the disease in the lung tissue. It gets the right answer (identifying the disease), but for the wrong reason (spotting the marker, not the disease characteristics).</p> <p>Short-cut learning is a significant challenge in AI because it can lead to models that are brittle, biased, or ineffective in real-world conditions, despite showing high accuracy in controlled or test environments. It underscores the importance of robust, diverse datasets and the need for explainability in AI models to ensure that they are learning and reasoning correctly.</p>"},{"location":"intro/03-white-box-vs-black-box/","title":"Comparing White Box and Black Box Machine Learning Models","text":""},{"location":"intro/03-white-box-vs-black-box/#introduction","title":"Introduction","text":"<p>In the ever-evolving world of machine learning (ML), understanding the nature and implications of different model architectures is crucial. This chapter delves into the comparison of white-box and black-box machine learning models, exploring their pros and cons, the tools used to build each, and the spectrum of explainability among various models.</p>"},{"location":"intro/03-white-box-vs-black-box/#white-box-machine-learning","title":"White Box Machine Learning","text":"<p>White box models are characterized by their transparency and interpretability. They allow users to understand how input data is transformed into predictions or decisions.</p> <p>Pros:</p> <ul> <li>Transparency: The decision-making process is clear, making it easier to trust and validate the model.</li> <li>Debugging and Improvement: Errors can be traced and understood, allowing for targeted improvements.</li> <li>Regulatory Compliance: In industries with strict regulatory requirements, white box models are often essential.</li> </ul> <p>Cons:</p> <ul> <li>Simplicity and Limitation: These models may struggle with complex data patterns that more sophisticated black box models can capture.</li> <li>Computationally Less Efficient: Some white box models, particularly those handling large datasets, can be less efficient.</li> </ul> <p>Tools and Techniques:</p> <ul> <li>Linear Regression: Used for predicting a continuous outcome.</li> <li>Decision Trees: Offer a clear decision-making path, often visualized as a tree.</li> <li>Rule-Based Systems: Systems that apply a set of pre-defined rules to make decisions.</li> </ul>"},{"location":"intro/03-white-box-vs-black-box/#black-box-machine-learning","title":"Black Box Machine Learning","text":"<p>Black box models are often more complex, where the internal decision-making logic is not transparent or is too complex to be easily understood.</p> <p>Pros:</p> <ul> <li>Handling Complexity: They excel in capturing intricate patterns in large and complex datasets.</li> <li>**Higher Accuracy: Often provides higher predictive accuracy than simpler white box models.</li> <li>Flexibility: Can be applied to a wide range of problems and datasets.</li> </ul> <p>Cons:</p> <ul> <li>Lack of Transparency: The decision-making process is not easily interpretable, which can lead to trust issues.</li> <li>Difficulty in Debugging: It's challenging to understand and correct the model's reasoning.</li> <li>Potential for Bias: Without understanding the model's logic, hidden biases may influence outcomes.</li> </ul> <p>Tools and Techniques:</p> <ul> <li>Neural Networks: Particularly deep learning models, which use layers of interconnected nodes.</li> <li>Random Forests: An ensemble method that uses multiple decision trees.</li> <li>Support Vector Machines (SVMs): Effective for both classification and regression but can be opaque in their decision-making.</li> </ul>"},{"location":"intro/03-white-box-vs-black-box/#the-spectrum-of-explainability","title":"The Spectrum of Explainability","text":"<p>It's important to note that explainability in machine learning models is not binary but exists on a spectrum. Some models offer a balance between transparency and complexity.</p> <ul> <li>Ensemble Methods: Methods like Gradient Boosting can offer more interpretability than deep learning models but are less transparent than simple decision trees.</li> <li>Regularization Techniques in Linear Models: These can make models more interpretable by reducing overfitting and simplifying the model.</li> </ul>"},{"location":"intro/03-white-box-vs-black-box/#tools-for-enhancing-explainability-in-black-box-models","title":"Tools for Enhancing Explainability in Black Box Models","text":"<ul> <li>SHapley Additive exPlanations (SHAP): Uses game theory to explain the output of any machine learning model.</li> <li> <p>Local Interpretable Model-agnostic Explanations (LIME): Explains predictions of any classifier in an interpretable manner.</p> </li> <li> <p>Feature Importance Analysis: Helps in understanding the significance of different features in the model's decisions.</p> </li> </ul>"},{"location":"intro/03-white-box-vs-black-box/#conclusion","title":"Conclusion","text":"<p>The choice between white box and black box models involves a trade-off between interpretability and predictive power. White box models, with their transparency, are indispensable in situations requiring clarity and accountability. Black box models, despite their opacity, are invaluable for tackling complex, high-dimensional problems. The field of machine learning continues to evolve, striving to bridge the gap between these two paradigms, aiming for models that are both powerful and interpretable. Understanding the strengths, limitations, and appropriate applications of each model type is essential for practitioners in the field of machine learning and AI.</p>"},{"location":"intro/04-darpa-reference-model/","title":"DARPA XAI Model","text":"<p>The image above describes the DARPA reference model for explainable AI (XAI). We will look at this image and describe how the DARPA XAI model is different from the standard machine-learning process. We will then speculate if the box labeled \"Explainable Model\" could be created using a knowledge graph.</p> <p>The image depicts two different approaches to machine learning: the traditional approach and the DARPA's Explainable AI (XAI) approach.</p> <p>In the traditional machine learning approach:</p> <ol> <li>Training Data is used in the Machine Learning Process to create a Learned Function.</li> <li>This function is then used to make a Decision or Recommendation.</li> <li>The User then interacts with this decision or recommendation, often posing questions like:<ul> <li>Why did you do that?</li> <li>Why not something else?</li> <li>When do you succeed?</li> <li>When do you fail?</li> <li>When can I trust you?</li> <li>How do I correct an error?</li> </ul> </li> </ol> <p>This traditional model often acts as a black box, providing little insight into how the decisions are made, which can be problematic for users who need to understand the reasoning behind the AI's outputs.</p> <p>In contrast, the DARPA XAI model includes additional components for explainability:</p> <ol> <li>Training Data is still used in a New Machine Learning Process to create an Explainable Model.</li> <li>Along with the model, an Explanation Interface is also developed.</li> <li>The User then interacts with the model and interface, leading to a situation where the user can:<ul> <li>Understand why the decision was made (I understand why).</li> <li>Understand the reasoning for not choosing an alternative (I understand why not).</li> <li>Know when the model is likely to succeed or fail.</li> <li>Know when to trust the model's decision.</li> <li>Understand the reasons behind any errors (I know why you erred).</li> </ul> </li> </ol> <p>The XAI model is designed to be transparent and provide users with insights into the machine learning process. This is particularly important in fields where understanding the decision-making process is crucial, such as healthcare, autonomous vehicles, or any domain where trust and reliability are paramount.</p>"},{"location":"intro/04-darpa-reference-model/#using-a-knowledge-graph-as-the-explainable-model","title":"Using a Knowledge Graph as the Explainable Model","text":"<p>Regarding the creation of the \"Explainable Model\" using a knowledge graph: Yes, it's a great idea! A knowledge graph can be leveraged to create an explainable model by linking data points with their relationships and properties in a graph structure. This graph can then be used to trace the decision-making path of the AI, showing how various inputs and their relationships contribute to a particular decision.</p>"},{"location":"intro/04-darpa-reference-model/#queryability","title":"Queryability","text":"<p>A knowledge graph allows users to query the model and receive explanations in terms of the relationships and entities within the knowledge graph, which can be highly informative and interpretable. This approach is particularly useful for complex domains where numerous variables and their interconnections must be considered, like in financial risk assessment or personalized medicine.</p>"},{"location":"regulatory/","title":"Explainable AI in a Regulated Environment","text":""},{"location":"risk/","title":"Levels of Risk","text":"<p>In our course, we define five levels of risk related to making a prediction and needing a clear explanation of why a prediction was made.</p> <ol> <li>Passive Modification: AI agents perform a background process that modifies resources but does not involve high-stakes</li> <li>Assisted Decision-Making AI agents provide low-stakes recommendations where human confirmation is required before any action is taken.</li> <li>Opt-In High-Stakes: A high-stakes recommendation and humans must opt-in to accept a recommendation made by an AI agent</li> <li>Opt-Out High-Stakes: A high-stakes recommendation humans must opt-out to avoid a recommendation</li> <li>Autonomous High-Stakes: A high-stakes recommendation and there are no humans in the loop</li> </ol>"},{"location":"risk/fda-risk-model/","title":"FDA File Level Model","text":"<p>The five-level risk category system we described for AI predictions and explanations can be compared to the U.S. Food and Drug Administration's (FDA)regulatory framework for machine learning in medical devices. The FDA's approach, particularly in its guidance on the use of AI and machine learning in medical devices, tends to focus on the safety and effectiveness of these technologies in healthcare.</p> <p>The FDA's approach to regulating AI and machine learning in medical devices, as outlined in their Artificial Intelligence/Machine Learning Action Plan, does not specifically categorize risks into a five-level model like the one we use in this course. Instead, the FDA focuses on a holistic approach to oversight that encompasses the entire lifecycle of AI/ML-based medical software (SaMD). This includes developing a regulatory framework for software that learns over time, supporting good machine-learning practices, ensuring patient-centered approaches, and advancing real-world performance monitoring\u200b\u200b.  See FDA Releases Artificial Intelligence/Machine Learning Action Plan - January 12th</p> <p>In the broader context of AI in healthcare, the FDA's role is to ensure the safety and effectiveness of AI-enabled products under its jurisdiction. The FDA considers adapting its review process for rapidly evolving AI-enabled medical devices, particularly those that can change in response to new data in ways that are difficult to predict. This process involves dealing with challenges such as ensuring the algorithms are trained on large, diverse datasets to be generalizable and unbiased, and validating their performance in real-world settings\u200b\u200b.</p> <p>In contrast, our five-level risk category system appears to be more about the level of human interaction and oversight in decision-making, ranging from low-stakes assistance to high-stakes autonomous decision-making. While there are similarities in terms of escalating scrutiny and oversight based on risk, the FDA's current approach is more nuanced and specific to the complexities of AI/ML in medical devices, focusing on the totality of a product's lifecycle and the specific challenges of ensuring safety and efficacy in the rapidly evolving field of digital health.</p> <p>Here's a comparison:</p> <ol> <li> <p>Passive Modification: This level, where AI modifies resources without high stakes, can be likened to the FDA's least restrictive category, possibly falling under \"Software as a Medical Device\" (SaMD) with minimal patient risk. These are typically subject to general controls.</p> </li> <li> <p>Assisted Decision-Making: This level requires human confirmation for AI recommendations. It's similar to the FDA's approach for AI tools that assist healthcare providers in decision-making but don't directly impact patient care. These are closely monitored and may require specific controls to ensure safety and effectiveness.</p> </li> <li> <p>Opt-In High-Stakes: This is akin to more critical FDA-regulated AI applications where the recommendation of AI can significantly impact patient outcomes, but the final decision is still human-dependent. These devices may be subjected to more rigorous premarket review to ensure their safety and effectiveness.</p> </li> <li> <p>Opt-Out High-Stakes: This level parallels FDA-regulated AI applications that play a significant role in patient care, where the default is to accept the AI recommendation unless manually overridden. The FDA would likely require stringent premarket approval or de novo classification for such devices, ensuring high standards of safety and efficacy.</p> </li> <li> <p>Autonomous High-Stakes: This matches the FDA's most critical category, likely akin to AI systems that operate independently in making healthcare decisions or diagnoses without human oversight. These would require the highest level of regulatory scrutiny, including extensive clinical trials and premarket approval, to ensure they meet the stringent criteria for safety and effectiveness.</p> </li> </ol> <p>In summary, while the FDA's regulatory framework is specifically tailored to medical devices and focuses on patient safety and device effectiveness, the five-level AI risk category system you've described has a broader application but aligns well with the FDA's approach in terms of escalating oversight and scrutiny based on the level of risk and autonomy in decision-making.</p>"},{"location":"risk/fda-risk-model/#references","title":"References","text":"<ul> <li>How FDA Regulates Artificial Intelligence in Medical Products</li> </ul>"}]}