{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Explainable AI Course","text":"<p>In this course, we will cover the key concepts around building analytical systems that focus on explainability.  This is an important goal for many modern AI systems today.  Studies show that people are reluctant to use AI systems that they don't trust, and explainability is a key to building that trust.</p> <p>Please let me know if you have any feedback on this course and our classroom materials.</p>"},{"location":"contact/","title":"Contact","text":"<p>Please contact me on LinkedIn</p> <p>Thanks! - Dan</p>"},{"location":"glossary/","title":"Glossary of Terms for Explainable AI","text":""},{"location":"glossary/#clever-hans","title":"Clever Hans","text":""},{"location":"glossary/#human-in-the-loop","title":"Human in the Loop","text":""},{"location":"glossary/#level-of-risk","title":"Level of Risk","text":"<p>In our course, we define five levels of risk related to making a prediction and needing a clear explanation of why a prediction was made.</p> <ol> <li>Passive Modification: AI agents perform a background process that modifies resources but does not involve high-stakes</li> <li>Assisted Decision-Making AI agents provide low-stakes recommendations where human confirmation is required before any action is taken.</li> <li>Opt-In High-Stakes: A high-stakes recommendation and humans must opt-in to accept a recommendation made by an AI agent</li> <li>Opt-Out High-Stakes: A high-stakes recommendation humans must opt-out to avoid a recommendation</li> <li>Autonomous High-Stakes: A high-stakes recommendation and there are no humans in the loop</li> </ol>"},{"location":"glossary/#lime","title":"LIME","text":""},{"location":"glossary/#shap","title":"SHAP","text":""},{"location":"license/","title":"Creative Commons License","text":"<p>All content in this repository is governed by the following license agreement:</p>"},{"location":"license/#license-type","title":"License Type","text":"<p>Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED)</p>"},{"location":"license/#link-to-license-agreement","title":"Link to License Agreement","text":"<p>https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en</p>"},{"location":"license/#your-rights","title":"Your Rights","text":"<p>You are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format</li> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p>"},{"location":"license/#restrictions","title":"Restrictions","text":"<ul> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li> <li>NonCommercial \u2014 You may not use the material for commercial purposes.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> <li>No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.</li> </ul> <p>Notices</p> <p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.</p> <p>No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p> <p>This deed highlights only some of the key features and terms of the actual license. It is not a license and has no legal value. You should carefully review all of the terms and conditions of the actual license before using the licensed material.</p>"},{"location":"types-of-analysis/","title":"Types of Data Analysis Methods Ranked by Explainability","text":"<p>When evaluating the explainability of data analysis methods, it's important to consider how easily the results can be interpreted by humans. Here's an ordered list of the methods mentioned, along with a few additional common methods, ranked from most explainable to least explainable:</p> <ol> <li> <p>Decision Trees: Highly explainable due to their simple, rule-based structure that mimics human decision-making processes.</p> </li> <li> <p>Linear Regression: Highly interpretable because it models relationships linearly with clear coefficients.</p> </li> <li> <p>Logistic Regression: Similar to linear regression but used for classification problems. The coefficients provide direct insights.</p> </li> <li> <p>Knowledge Graph: Explainability depends on the complexity of the graph and the relationships modeled, but generally, they are quite interpretable due to their structured nature.</p> </li> <li> <p>Single Layer Neural Network: More complex than regression models but still relatively straightforward due to having only one layer of processing.</p> </li> <li> <p>Gradient Boosting with XGBoost: While feature importance can be extracted, the ensemble nature and complex interactions make it less interpretable than simpler models.</p> </li> <li> <p>Two Layer Deep Neural Network: Begins to enter the realm of deep learning, making it harder to interpret due to multiple layers of abstraction.</p> </li> <li> <p>Random Forest: An ensemble of decision trees, more accurate than a single tree but less interpretable due to averaging over many trees.</p> </li> <li> <p>Deep Neural Network (more than two layers): As the number of layers increases, so does the model's complexity, making it less interpretable.</p> </li> <li> <p>Convolutional Neural Networks (CNNs): Commonly used in image analysis. They are less interpretable due to their complex structure designed to pick up hierarchical patterns.</p> </li> <li> <p>Recurrent Neural Networks (RNNs): Especially ones with long short-term memory (LSTM) units. Used for sequential data like time series or text, they are quite complex and less interpretable.</p> </li> <li> <p>Transformer Models (like BERT, GPT): Very complex with millions of parameters, making them the least interpretable.</p> </li> </ol>"},{"location":"intro/","title":"What is Explainable AI?","text":"<p>Welcome to our course on Explainable AI (XAI), where we embark on a journey through the fascinating world of artificial intelligence (AI) that is not only powerful but also interpretable and transparent. This course is designed to unravel the complexities of AI, making it understandable and accountable to a wide range of stakeholders, from developers to end-users.</p>"},{"location":"intro/#defining-explainable-ai","title":"Defining Explainable AI","text":"<p>Explainable AI refers to methods and techniques in the field of artificial intelligence that make the outputs and operations of AI systems understandable to humans. Unlike traditional AI systems, where the decision-making process can be opaque, XAI aims to create a transparent relationship between the AI's functionality and its decision-making process.</p>"},{"location":"intro/#comparing-systems-simple-vs-deep-learning-systems","title":"Comparing Systems: Simple vs. Deep-Learning Systems","text":"<p>To grasp the essence of XAI, it's crucial to understand the contrast between simple, easy-to-explain systems and complex deep-learning systems. Simple AI systems, like decision trees, are inherently transparent with clear-cut rules and predictable outcomes. On the other hand, deep-learning systems, utilizing neural networks with multiple layers, are akin to a \"black box.\" They can process an immense amount of data and find patterns beyond human capabilities, but their internal workings are often not easily interpretable.</p>"},{"location":"intro/#white-box-vs-black-box-models","title":"White Box vs. Black Box Models","text":"<p>This brings us to the concepts of \"White Box\" and \"Black Box\" models in AI. White Box models are those where the internal logic is fully transparent and understandable to humans. In contrast, Black Box models are those where the decision-making process is not visible or understandable, making it challenging to decipher how the AI arrived at a specific decision.</p>"},{"location":"intro/#the-dangers-of-non-explainable-systems","title":"The Dangers of Non-Explainable Systems","text":"<p>The inability to understand or predict the behavior of AI systems poses significant risks. It can lead to a lack of trust, incorrect or unethical decision-making, and difficulties in identifying and correcting errors. In domains like healthcare, finance, and law, where decisions have profound impacts, the opaqueness of Black Box models can be particularly perilous.</p>"},{"location":"intro/#tools-to-classify-risk-in-ai-systems","title":"Tools to Classify Risk in AI Systems","text":"<p>To mitigate these risks, it's vital to have tools that can classify the risk associated with AI decisions. These tools assess factors like the impact of decisions, the complexity of models, and the possibility of bias, providing a framework to evaluate the potential harm or unintended consequences of AI decisions.</p>"},{"location":"intro/#making-deep-neural-networks-explainable","title":"Making Deep Neural Networks Explainable","text":"<p>Lastly, we will explore the tools and techniques to make deep neural networks more explainable. Techniques like Layer-wise Relevance Propagation (LRP), SHAP (SHapley Additive exPlanations), and attention mechanisms can help illuminate the inner workings of complex models. We will delve into how these tools break down the AI's decision-making process, making it more transparent and understandable.</p> <p>Throughout this course, we will explore these concepts in detail, equipping you with the knowledge to create, analyze, and advocate for AI systems that are not just powerful, but also responsible and explainable. Welcome aboard this exciting exploration into the world of Explainable AI.</p>"},{"location":"intro/02-clever-hans/","title":"Clever Hans","text":"<p>The Tale of Clever Hans, the \"Mathematical\" Horse</p> <p></p> <p>Once upon a time in Germany, around the early 1900s, there lived a horse named Hans. But he wasn't just any horse; he was \"Clever Hans,\" a horse with a knack for numbers---or so people thought. Hans was owned by Wilhelm von Osten, a school teacher and horse trainer who claimed that Hans could understand German and solve complex mathematical problems. Imagine a horse doing your math homework!</p> <p>Von Osten would ask Hans questions like \"What is two plus three?\" and Hans would tap his hoof five times. The crowd would go wild! \"Ein Genie!\" they exclaimed, which means \"A genius!\" in German. Hans became a celebrity, with people coming from far and wide to see the brainy horse who could count.</p> <p>But, as with all great mysteries, there was a twist. A psychologist named Oskar Pfungst became curious about Hans's abilities. Pfungst conducted a series of experiments and discovered something astonishing. It turned out Hans was not exactly a mathematician; instead, he was incredibly good at reading human body language!</p> <p>When von Osten or the audience knew the answer to a question, they would, without realizing it, give subtle, unintentional cues. A slight nod, a change in posture, or an eager look in their eyes. Hans was picking up on these tiny signals, and when the signals stopped---like when the audience held their breath in anticipation---he would stop tapping.</p> <p>Hans was not a math whiz, but he was a master of observation, a Sherlock Holmes of the equine world!</p> <p>Relating Clever Hans to Explainable AI</p> <p>The story of Clever Hans is a humorous yet enlightening prelude to the importance of explainable AI. Just like how people believed Hans understood math, we often assume AI systems make decisions based on logical processing of the given inputs. However, sometimes, these systems might be 'tapping their hoof' based on subtle, unintended patterns in the data, not unlike Hans responding to unconscious human cues.</p> <p>In AI, this is akin to a model picking up on noise, biases, or irrelevant patterns in the data, leading to correct outcomes for the wrong reasons. Just as Pfungst's investigation revealed the true nature of Hans's abilities, explainable AI aims to dissect and understand the 'how' and 'why' behind AI decisions. It's crucial to ensure AI systems are making decisions for the right reasons, especially in critical applications like healthcare, finance, and law.</p> <p>So, the next time you encounter a seemingly \"intelligent\" AI, remember Clever Hans and ask yourself, \"Is this AI genuinely smart, or is it just good at picking up on the digital equivalent of a nod and a wink?\" The quest for explainable AI is, in essence, the pursuit of ensuring our AI systems are more than just clever at tapping hooves, but truly understanding and solving the problems at hand.</p>"},{"location":"intro/02-clever-hans/#short-cut-learning","title":"Short-Cut Learning","text":"<p>\"Short-cut learning\" in the context of artificial intelligence and machine learning is a phenomenon where a model achieves the correct answer or output, but it does so for the wrong reasons. This concept is a bit like a student who correctly guesses the answers on a test without actually understanding the subject matter.</p> <p>In more technical terms, short-cut learning occurs when a machine learning model picks up on superficial, spurious, or irrelevant patterns in the training data that happen to correlate with the right answers. These patterns are not the underlying causal factors that a human expert would use to make a decision. As a result, while the model may perform well on the training data or even some testing scenarios, its understanding is shallow and may fail in real-world situations where the spurious patterns it learned do not apply.</p> <p>For example, imagine a medical AI trained to detect a certain disease from X-ray images. If most images of diseased lungs also coincidentally have a small marker or label on them that healthy lung images do not, the model might learn to identify the disease by looking for this marker instead of actual signs of the disease in the lung tissue. It gets the right answer (identifying the disease), but for the wrong reason (spotting the marker, not the disease characteristics).</p> <p>Short-cut learning is a significant challenge in AI because it can lead to models that are brittle, biased, or ineffective in real-world conditions, despite showing high accuracy in controlled or test environments. It underscores the importance of robust, diverse datasets and the need for explainability in AI models to ensure that they are learning and reasoning correctly.</p>"},{"location":"risk/","title":"Levels of Risk","text":"<p>In our course, we define five levels of risk related to making a prediction and needing a clear explanation of why a prediction was made.</p> <ol> <li>Passive Modification: AI agents perform a background process that modifies resources but does not involve high-stakes</li> <li>Assisted Decision-Making AI agents provide low-stakes recommendations where human confirmation is required before any action is taken.</li> <li>Opt-In High-Stakes: A high-stakes recommendation and humans must opt-in to accept a recommendation made by an AI agent</li> <li>Opt-Out High-Stakes: A high-stakes recommendation humans must opt-out to avoid a recommendation</li> <li>Autonomous High-Stakes: A high-stakes recommendation and there are no humans in the loop</li> </ol>"},{"location":"risk/fda-risk-model/","title":"FDA File Level Model","text":"<p>The five-level risk category system we described for AI predictions and explanations can be compared to the U.S. Food and Drug Administration's (FDA)regulatory framework for machine learning in medical devices. The FDA's approach, particularly in its guidance on the use of AI and machine learning in medical devices, tends to focus on the safety and effectiveness of these technologies in healthcare.</p> <p>The FDA's approach to regulating AI and machine learning in medical devices, as outlined in their Artificial Intelligence/Machine Learning Action Plan, does not specifically categorize risks into a five-level model like the one we use in this course. Instead, the FDA focuses on a holistic approach to oversight that encompasses the entire lifecycle of AI/ML-based medical software (SaMD). This includes developing a regulatory framework for software that learns over time, supporting good machine-learning practices, ensuring patient-centered approaches, and advancing real-world performance monitoring\u200b\u200b.  See FDA Releases Artificial Intelligence/Machine Learning Action Plan - January 12th</p> <p>In the broader context of AI in healthcare, the FDA's role is to ensure the safety and effectiveness of AI-enabled products under its jurisdiction. The FDA considers adapting its review process for rapidly evolving AI-enabled medical devices, particularly those that can change in response to new data in ways that are difficult to predict. This process involves dealing with challenges such as ensuring the algorithms are trained on large, diverse datasets to be generalizable and unbiased, and validating their performance in real-world settings\u200b\u200b.</p> <p>In contrast, our five-level risk category system appears to be more about the level of human interaction and oversight in decision-making, ranging from low-stakes assistance to high-stakes autonomous decision-making. While there are similarities in terms of escalating scrutiny and oversight based on risk, the FDA's current approach is more nuanced and specific to the complexities of AI/ML in medical devices, focusing on the totality of a product's lifecycle and the specific challenges of ensuring safety and efficacy in the rapidly evolving field of digital health.</p> <p>Here's a comparison:</p> <ol> <li> <p>Passive Modification: This level, where AI modifies resources without high stakes, can be likened to the FDA's least restrictive category, possibly falling under \"Software as a Medical Device\" (SaMD) with minimal patient risk. These are typically subject to general controls.</p> </li> <li> <p>Assisted Decision-Making: This level requires human confirmation for AI recommendations. It's similar to the FDA's approach for AI tools that assist healthcare providers in decision-making but don't directly impact patient care. These are closely monitored and may require specific controls to ensure safety and effectiveness.</p> </li> <li> <p>Opt-In High-Stakes: This is akin to more critical FDA-regulated AI applications where the recommendation of AI can significantly impact patient outcomes, but the final decision is still human-dependent. These devices may be subjected to more rigorous premarket review to ensure their safety and effectiveness.</p> </li> <li> <p>Opt-Out High-Stakes: This level parallels FDA-regulated AI applications that play a significant role in patient care, where the default is to accept the AI recommendation unless manually overridden. The FDA would likely require stringent premarket approval or de novo classification for such devices, ensuring high standards of safety and efficacy.</p> </li> <li> <p>Autonomous High-Stakes: This matches the FDA's most critical category, likely akin to AI systems that operate independently in making healthcare decisions or diagnoses without human oversight. These would require the highest level of regulatory scrutiny, including extensive clinical trials and premarket approval, to ensure they meet the stringent criteria for safety and effectiveness.</p> </li> </ol> <p>In summary, while the FDA's regulatory framework is specifically tailored to medical devices and focuses on patient safety and device effectiveness, the five-level AI risk category system you've described has a broader application but aligns well with the FDA's approach in terms of escalating oversight and scrutiny based on the level of risk and autonomy in decision-making.</p>"},{"location":"risk/fda-risk-model/#references","title":"References","text":"<ul> <li>How FDA Regulates Artificial Intelligence in Medical Products</li> </ul>"}]}